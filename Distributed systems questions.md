---
tags:
  - distributed_systems
---
### Describe the REST architectural style

The REST style is s way to build a client server application that communicates under HTTP/HTTPS via JSON/XML messages. This is because REST only offers four methods for communication: PUT, POST, GET, DELETE; thus there isn't enough methods to build an entire application. The solution to this problem is to encode the signature and arguments of a function is a JSON/XML message and send that through one of the four methods listed before. The other main point of REST is that only the client should hold the state of the connection, thus the server shall have no such concept. In reality a pure REST application is hard to implement and wouldn't really meet the needs of the users, for this reason usually the server will hold some information of the client in persistent storage like a database.

All the responses that the client receives can be specified to be cached.
### Describe name resolution in general and focus on the various approaches you know to resolve flat names.

Names are used to refer to entities in a system, there are two types of names: human friendly -- usually represented as strings -- and machine friendly - -represented as ips.  Name resolution stands for the process of converting a human friendly name to the actual address of the machine. Name resolution can happen in three main methods: flat naming, structured naming and attribute based naming.

In flat naming every machine has it's own name, usually without any meaning or structure to it represented with a string. There are various versions for resolution of flat names, for simple solutions like home networks we can just broadcast the network or in some cases even use multicast to find the device. To handle mobile devices we can use forwarding pointers that transfer the reference of the device between routers when in motion. There are also home based approaches that works by having a "home server" and when a client needs to find another node in the network, it can just contact this server that know where everybody is, the problem with this approach is that the server location is fixed, so requests may have to travel a lot even if the two devices are not very far from one another. An extension of a home based approach is to use a dth that divides the network into various subnetworks and knows the address of everybody in it's own network, if a client needs to communicate with someone outside the network, the request will navigate between the various dth s until it finds the desired network and then it gets to the receiver. As a last technique we have hierarchical naming, with this approach the names have some structure and are organized as a tree like a directory in a file system, and every non leaf node knows of every children node, this has obvious bad implications, in fact the top level root node has information about every node in the network.
### Describe leader election protocols and compare them in terms of assumptions and number of messages required to end the election

In a distributed system leader election can happen in two ways: bully election and ring based. Both of these protocol require a reliable connection and a unique id that distinguishes them from every other node. Also we have to treat the system as if it was a synchronous one, so recognizing that someone has crashed needs to be possible.

The bully election protocol works by trying to elect the highest id node in the network as a leader, this happens in a few steps: first a node has to notice that a node is unresponsive, that the same node will try to contact every other node that it knows has higher id that them and waits for a response. If it receives no response then it will assume that it's the new leader of the network and send a new message notifying every other node of this change. 
In contrast the ring based protocol it arranges the nodes in a logical loop and still tries to elect a new leader with highest id. When a process notices that the leader is non respondent, it will send a message containing its id to the next node in the chain, if a node doesn't respond then it will try the next one and so on, then the receiver will add it's own id to the message and pass it along. If a node receives a message containing it's own id, then it will select the highest id node and pass the information along to all the nodes.
### Consider the MapReduce programming model and the execution framework:

1) Explain the programming primitives that the model offers.
	The model offers a two primitives: a map function that represents a pure function that applies a functor to some data, and then it returns the result, and the reduce that takes as input the list of result of the map computation and applies a "reducing" function to it then returning only a single result.
	
2) Exemplify these programming primitives using a word/count application that takes in input some documents and outputs the number of occurrences of each word appearing in the documents.
	In this kind of system we can view the word counting function as the map function that takes in input a file and then it will return a map that connects every word with its number of occurrences. Then we can view the reduce as the operation that takes in all the various maps, given as a result of every computation of every file, and it will combine them all into one
	
3) Explain how the execution framework handles data parallelism, data locality, fault tolerance and stragglers.
	Each computation can be done in parallel since the function that both the map and the reduce use are pure, this means that the result of their computation depends only on their input. This property makes the system massively parallelizable. But when having such a parallel application we have to think about the data locality, because transfer speeds are usually much slower than any computation, this imposes a limit on how distant data can be from one another, usually data is contained in the same data center or even in the same rack. To handle all this processes we use a node that act as a master that checks if some nodes are slower or even crash, if they are slower than others they are called stragglers and they will be given less amount of work, this is because if they had the same amount of work as some other node the whole computation will be limited just by the speed of this node. The objective of the master is also to find the crashes in the system, in one is detected then the master will ask to redo all the work to some other node, if by any chance the node that has been considered crashes responds with a result, the message will be ignored.
### Describe the following three approaches for search and lookup in a p2p network. Compare them in terms of state maintained at each node, search expressivity, search scope, guarantees of lookup success, resilience to network dynamicity (joins and leaves of nodes)

-  Centralized table (e.g. Napster)
	When there is a centralized table there is a node in the network that holds the location of every other node in the network and what each node is posting. Each node will only hold information regarding the location of the central table and what resources it makes available. A node can easily search for a resource in the entire network just by asking the central directory if it holds the reference to a node that provides it, so there are no guarantees of success. New nodes to subscribe to a network, a new node just has to contact the central server that will respond with the list of the documents that the network offers.

- Flooding (e.g. Gnutella)
	In query flooding there is no concept of a centralized or semi centralized resource that holds information about the system, to find a resource you have to ask to every node you can reach and if they don't have it, they will forward the request to each of their peers. This method is extremely wasteful and does not guarantee anything about whether or not a resource is available on the network, to find out you have to ask everyone. This approach is also very sensible of network dynamicity because if a node leaves it can divide the network in two separate network without no one noticing, though this eventuality is quite unlikely in a big network since when a node tries to connect, it has to inform every node on the network and the most free one will establish a connection between them. The probability of accepting a connection is inversely proportional to the number of active connections.
 
- Distributed Hash Table (DHT, e.g. Chord)
	Chord is a middle ground with the centralized hash table approach and the query flooding one. There isn't only a central node that holds the references of all nodes in the network, but there are multiple tables that divide the network based on the hash of the resource they offer. Each table holds the position of every other table in the network. When entering this network a client just needs to contact a node that will lead it to the right hash table to register to the network. When searching one needs to contact a dth based on the computed hash of the search and ask it if it holds the resource, if the resource isn't available then we are sure that is not available in the entire system.
### Describe the various mobile code paradigms and the type of technologies that support them? Which paradigm is the most complex to implement? Why?

There are four types of mobile code paradigms: client server -- where the client makes a request to the server and then the server will respond with some kind of result, all the computation will happen in the server and the client doesn't know how the result is produced -- this type architecture is used in most web application and api; remote evaluation -- where the client tells the server what operation it should execute then the server will reply with the result -- that is used to offload computation like in a scientific environment; code on demand -- where the client will ask the server how it should handle a computation, the server will respond with the actual code -- and is mostly used by web browsers that download javascript bundles that will run on the client; and lastly mobile agent -- a client will create a connection with a server and use it's resources as if it was using its own, all the result will remain on the server; The more a system is mobile, the more complex it is to implement, this means that the mobile agent is the most complex to maintain of them all, preceded by remote evaluation, code on demand and client server in order.
### Describe the difference between flat naming system implemented using hierarchy of servers and structured naming implemented using similar hierarchy or servers. Compare the two solution and explain why the latter is more efficient then the former.

Both hierarchical flat naming and structured naming compose the network in a tree-like structure, dividing the network between nodes. The main difference between them is the name resolution procedure, the former has to potentially ask every root node if it has the desired destination, and if it hasn't, then it has to ask their root and so on, so it's a bottom up approach where every root node has to know every node in it's subtree. The latter is a top down approach that in which, a root node may not know where the exact destination is, but it knows where to ask for the information, this means that the root only has to know about its leaves and the lookup speed is consistent because it has to ask at most h -- height of the tree -- nodes to find the destination.
### Consider client centric consistency models. In which situation are they relevant? Define the 4 models presented in class and discuss how can they be implemented

Client centric consistency models are relevant when a distributed system wants to serve multiple clients that are very far apart geographically; in such cases the main bottleneck will become the transfer speed of the network, which is the speed of light. To resolve this latency issue we can replicate the systems' data in various servers and keep them in sync with each other, this is for example how cdn work. To implement such systems we can use one of these different models:

Sequential consistency $\to$ All operations by all processes must be executed as if they were executed in some sequential order and each operation must appear in the same sequence as specified by its program. This is not a good approach since it basically replicates a synchronous system, thus including massive latency and as such, the system wouldn't be highly available. It can be implemented either with a single server that acts as a leader that dictates the single source of truth, thus all the write operations must be sent to the leader, or in a leaderless context with a quorum that votes on the changes made by the system.

Linearizable consistency $\to$ All write operations should be considered as immediately visible to all the other replicas. This system uses vector clocks to maintain timestamps of the state of the system. Only the leader can commit new information to the system and when it does, it must lock all replicas to perform an atomic update.  This kind of system is hard to maintain in the presence of failures.

Causal consistency $\to$ All write operations that may be causally related must be seen by all processes in the same order based on the notion of Lamport's happens before relationship. This type of system can be implemented with a multi leader system that keeps timestamps of each write operation and commits to persistent storage only if all possible causes of a write are processed.

FIFO consistency $\to$ Writes done by a single process are seen by all others in the order in which they were issued; writes from difference processes may be seen in a different order at different machines. This can easily be implemented with multi leader protocol by committing changes only if all other changes from the sender process were applied.
### With reference to big data processing platforms: explain the difference between pipelined and scheduled execution and discuss how the two execution models influence the parallelism, load balancing and elasticity

In big data platforms the work is usually divided between multiple processes, in order to handle the work we usually use the map reduce pattern. With this pattern data can be distributed to a lot of nodes in the network to be processed independently from one another. In fact, both of the map and reduce operation must be pure functions. To divide the work into multiple processes, a scheduler is used and it divides the work in chunks and sends it to a process to be processed. This kind of scheduling only performs one type of operation in the mapping stage, so it can create a lot of redundant work with the reduce operation and also a lot of redundant traffic. To resolve this problem we can apply a pipelining approach to all the operations, thus using the intermediate results to perform multiple operations before reducing the result. Both of these approaches are massively parallelizable, with an advantage to the pipelined execution for having way less network request, thus higher throughput. The problem with a pipelined approach arises when dealing with load balancing, and as a consequence elasticity. This is because since all operation are done by a single process, if that very process is slow, then it will still have to do all the work and that may impact the performance of the system. This problem doesn't exist with a scheduled approach because, since the scheduler asks only to process one operation at a time, the whole computation takes a lot less and if a client is identified as slow, then we can just redirect the computation to other processes.
### Describe publish/subscribe in detail discussing the various alternatives for the subscription language, then describe and compare the various approaches to implement a distributed (acyclic) dispatcher

There are two types of languages to use when subscribing to a publish subscribe network: subject based and content based. The former describe the content by assigning it a subject from a predetermined set of subjects and all the clients that subscribe to that subject will receive a notification. The latter describes the content, dividing in key value pairs the type of content of the resource and sending all interested clients a notification. We can implement such system in three different way in a acyclic dispatcher: 

Message forwarding $\to$ When a broker receives a message it forwards it to the interested clients that are connected to it, and then it forwards it to all other brokers in the network. There are no difference in this implementation between content based and subject based

Subscription forwarding $\to$ All the brokers know the shortest path to find all the interested clients, when it receives a message about a subject, a broker forwards it using that information to the next broker, or if available, to the interested client. Both approaches are possible with subscription forwarding, but since content based will cause a lot of subscriptions, this approach doesn't work well since it would be faster to just broadcast the message.

Hierarchical forwarding $\to$ All network assumes a overlay tree, all of the subscription flow towards the nodes in the tree only if they are interested in the message.
### Describe and compare the various approaches you know to implement flat naming

In flat naming, the clients are represented by a human readable string and are almost always meaningless. There are five main approaches to achieve name resolution:
Broadcast/Multicast $\to$ Ask every /some node in the network if it's the desired node. This is only good for small networks, like LAN
Forwarding pointer $\to$ This approach is used with mobile clients that may switch access point during the connection, in this case the connection is passed between servers seamlessly from the client
Home based approach $\to$ There exists a known home server that every node queries when it wants to find someone else in the network, this also works for larger networks, but still has problems with latency due to geographic distance between the home and the clients.
DHT $\to$ There is a network tree overlayed to all nodes in the network, when a clients wants to find another it asks the information to the relevant hash table. 
Hierarchical flat naming $\to$ This approach is the exception to the "meaningless" string name. An example of this kind of approach are directories in a file system. Each node has knowledge of all of its children and when a node needs to find another one it asks its parent node, and if it doesn't find it ask his parent and so on, that will return the address of the node if it finds it.
### Describe the floodset algorithm. Which problem does it solves? Under which assumptions? May you provide a proof of its correctness

The floodset algorithm is used to implement consensus protocol that works under the assumption of process failures, but not byzantine failures. It works by repeating the computation on multiple processes and, if they all yield the same result, then we can accept it, if even one of them crashed, then the computation is discarded. 
### Describe the range of solutions in p2p systems regarding the problem of searching for items in the peers network: are there solutions offering any guarantees of search time?

Peer to peer systems can be implemented mainly in three approaches: with a centralized table, a dth or a fully decentralized.
Search lookup in a centralized table involves only querying that specified table and seeing if it returns some peers that offer that item. This has a guarantee of search time since it will take at most the time of a lookup. In a dht approach we have more tables that are distributed over the network, but since they are organized by their hash value, we can just interrogate the relevant tables when searching for something. This has less guarantees about time since it has to find the right hash table and depending on the implementation, for example a finger table, it can involve some jumps, but once it finds the right one it's just a matter of a lookup. Lastly in a fully decentralized network we have no idea where the item might be, nor where information about said item resides, the only thing we can do is flooding the network RPC is a protocol for network communication between clients that works by masking network requests under seemingly normal procedures. In with the request and hope it will find it, this approach doesn't have any guarantees about time, since it has to potentially check all the nodes in the network either directly or indirectly.     
### Describe and compare RPC and RMI (as general communication abstractions). Clarify the differences regarding the way parameters are handled

RPC is a protocol for network communication between clients that works by masking network requests under seemingly normal procedures. In fact the idea of this protocol is to be able to call a server function directly from a client as if it was a normal function. This approach poses a couple of limitations but the most important one is that there can be no references/pointers between the calls. This is because we aren't operating in a shared memory model anymore so two clients see two different blocks of memory, thus all parameters are passed by value. RPC is mostly a blocking API, this is on purpose since it tries to imitate a normal serial computation, but it can be rendered asynchronous. RPC was made for C and as such there is no concept of an object, inheritance exceptions and more modern concepts, to fix this RMI was created. It's the object oriented way to do RPC that implements such features and, depending on the implementation, can also support passing by reference some objects.
### Describe leader election. The goal, possible usage scenarios, how to implement it, under which hypothesis the presented protocol works

Leader election is a mechanism designed to maintain a leader in a distributed system. This leader will act as a coordinator for the entire system. This mechanism work under the assumption that a process may fail, and if the failure happens at the leader the entire system will react accordingly and elect a new one. There are two main approaches to achieve this and work under the hypothesis of a reliable connection and distinguishable clients, this is achieved by using unique identifiers, and those are:
bully election $\to$ when another server notices that the leader has crashed, be it through pings or a stale request, it sends a message containing its id to all the other nodes in the network that have higher id then itself. When a node receives this message it repeats it until there is a server that can't because it's the highest id. Once the highest id receives the message it will be elected as the new leader and it will send it's id to all other nodes in the network notifying them of the decision. 
ring election $\to$ this approach also has another assumption, the network has to be arranged topologically as a ring. when a client notices that the leader is unresponsive it will send a message containing it's id to next node in the chain, if even that is unresponsive then it will try with the next one and so on. When a node receives this message it appends its own id and forwards it onward. Once a server receives a message with its own id in it, this means we have done a complete loop, it will chose the highest node in the network as a coordinator, change the type of message as COORD and it will pass it along to make another loop.
### Discuss virtual Synchrony.

Virtual synchrony is used in the context of fault tolerance of a distributed system. It's built upon the fact that close synchrony -- everyone sees the same messages or same changes of the view in the same order  -- in an environment with process failures is not possible. Virtual synchrony works by creating epochs whenever there is a change in membership. Whenever a message is sent, the sender only sees the a group of active processes that can receive a message and, after it gets sent, there is no guarantee to whether or not it gets received. These changes in membership can be seen as a multicast message that either notifies of a new process or an old one that has exited.
### Describe and compare the data-centered (Linda) model of communication with the event-based one.

In data centered communication the data is kept in a shared persistent data space called the tuple space, because all the data is kept in tuples. To access the data space you have to communicate with the server using synchronous RPC to either put, read or remove something. These kind of systems do not scale well and, as described by the available operations, the system is not reactive at all.  On the other hand event based communication are a type of p2p architecture that can easily scale to handle a lot of clients. It works by dividing the networks into brokers and clients, the clients can subscribe to topics, that can be based on their category or on their content, and when a new clients publishes something to a topic, all the clients interested will be notified. There are a couple types of event based architecture depending on the type of network -- acyclic or cyclic -- and all of the communication primitives are non blocking and use TCP communication.
### Describe structured naming in general and DNS in particular

In structured naming, the clients are organized in a name space that can represent a graph or a tree structure. Each graph is composed of either leaf nodes or directories that contain the reference to other nodes and only know about the immediate successors, on the other hand a leaf represents a named entity that can be referred either via absolute or relative paths. The level of a directory also implies a certain degree of stability, this means that rarely can changes happen to it or it's immediate children. This is also the case with DNS, a protocol for name resolution used to access the web. In DNS the directory servers are divided in three main groups depending on their level:
- Global level $\to$ these are very few and are spread worldwide with many replicas for each of them. Also to avoid congestion all the updates are applied lazily
- Administrational level $\to$ These are mid level directories that can be managed each group by one administration, and as such are less spread geographically
- Managerial level $\to$ These compose the vast majority of servers that represents each its own administration
These differentiation also impacts how client side caching should work, the more stable the server is, the more caching can be done, and as such, only the global level is usually cached. Name resolution procedures, assuming a clean cache, always starts from the root level asking for a specific entity, the root level will point to one of its direct successors that will repeat the process until it finds or it doesn't find the entity that the client was looking for. These procedure can happen either iteratively or recursively, when done recursively the client only sends out one request and all the work is done by the servers that communicate with one another, when done iteratively all the requests get back to the client that has to send a new message to the lower level server until it finds the entity it was looking for. The main difference in the approaches is how caching should work, usually the recursive implementation can have a higher degree of caching and, as such, can find the entity in less requests than the counterpart.
### Describe mutual exclusion. Define it in precise terms and show the fully distributed protocol to achieve it using scalar clocks (clarify the assumptions for the protocol to work correctly)

Mutual exclusion is the practice of locking a resource for usage by only one process at a time. This type of synchronization paradigm works under the assumption of both reliable communication and processes. To implement such primitives we can use scalar clocks. Each process will hold its own timestamp that will be updated upon the arrival of a new message, this will provide the an approximation of causality withing a system. This means that whoever is the first who tries to access the shared resource will have its turn before all the other processes, this is achieved through the use of a queue that will add new requests to wait for their turn if the resource is already in use.
### Describe what is access control, how it works and the two main ways of implementing it in a distributed system

Access control describes the rules that processes have to follow when accessing a shared resource. This is achieved by means of mutual exclusion. Mutual exclusion works by locking a resource depending if some other process is already using it. There are a few methods to implement this, but the most important ones are with clocks, more precisely scalar clock that try to define a total ordering of the system and the other is with a token ring that defines the order of operation by organizing the system in a logical ring and passing a token that grants access to the shared resource in a loop.
### Describe the mobile code architectural style.

Mobile code defines a code architecture style in which the system is able to relocate the components of a distributed system at runtime. There are 4 main approaches to implement such systems:
- Client server $\to$ the client asks the server for something, the server will respond with the result of the computation
- Remote evaluation $\to$ the client sends the code for the computation to the server, it will run it and respond with the result
- Code on demand $\to$ the client asks the server for the code and after executes it locally, like javascript when visiting a web page
- Mobile agent $\to$ the client will take control of some resources of the server and execute the requests directly. No result will ever be send to the client
### Describe the various techniques to remove unreferenced entities in a distributed system. 

To remove unreferenced entities we can use one of these approaches:
- mark and sweep $\to$ the system is put on hold and, traversing all nodes in the network, the server will keep track of all the clients encountered, those will not be removed from the system, but all the others not encountered will. This is not a good approach for a distributed system since it needs to halt the entire system to run this computation
- Reference counting $\to$ Each node keeps track of how many other nodes have its reference, when adding a reference the count goes up by one and when removing it goes down by one. When reaching zero the node will remove itself from the system. This is not immune from race conditions so it's not a good approach either
- Weighted reference counting $\to$ This is version of reference counting in which a node only keeps track of the decrements. This is done by using two numbers: total weight and partial weight, these numbers have to be the same. When creating a new node the partial weight gets halved and when a computation terminates it sends it back. We can safely remove the node from the system when the partial weight and the total weight are the same.
- Reference listing $\to$ A node keeps track of the identities that are connected to the node through pings, once it finds itself isolated that it will stop. This still suffers from race conditions.
### Describe and discuss the protocols you know to get reliable group communication in case of reliable nodes and unreliable links

There are two main approaches to achieve reliable group communication in case of unreliable links and reliable nodes:
- Multicast $\to$ All messages in a communication are numbered and then multicasted to each node in the system, when a message is received the node it increments its own counter and then sends an ACK to the sender. This approach is prone to network congestion, thus in reality the opposite approach is used. When a node notices that it has received messages out of order, or it has skipped one, it starts a timer; once the timer has ended it will broadcast a NACK to the network that will try to resend the lost message in order to realign the node.
- Hierarchical feedback control $\to$ This is a more scalable approach in which the network is divided in local networks that can communicate between them through coordinators. It's needed for this link that connects coordinators to be reliable. Inside the local networks the nodes will use multicast.
### Describe and discuss the kind of failures (the “failure model”) that may happen in a distributed system.

There can be 3 type of failures inside a distributed system:
- An omission failure $\to$ a node has received a response from another node, this can be due to the message not arriving, the other node crashing or even the response getting lost
- Byzantine failure $\to$ a node is sending back a wrong result, be it due to a missing computation or an out of order one
- Timing failure $\to$ time limits of computations are violated

There is also another type of differentiation of failures based on their occurrence:
- Transient $\to$ these are one off errors and then never reappear
- Intermittent $\to$ these keep appearing and disappearing without any reason to do so
- Permanent $\to$ the failure persists
### Describe the problem of removing unreferenced entities in a distributed system and possible solutions to such problem.

A distributed network is vulnerable to network compartmentalization and failures, this means that some nodes may keep running without any reason to do so, this is because they are no longer reachable from the root node of the system. To deal with this problem there are a couple of solution:
- Mark and sweep $\to$ first thing, the system halts, then the root sends a message in broadcast to check which are the reachable nodes in the system. Once this computation is finished, then the remaining nodes will be removed. This assumes that the root holds a reference of all the nodes in the system.
- Reference counting $\to$ all nodes keep track of the number of reference they gave out, once the count reaches 0 then the node will leave. This is vulnerable to network partitioning and race conditions
- Weighted reference counting $\to$ This tries to solve the problems of reference counting by keeping track only of the decrements. Each node holds a total and partial count, when a node asks for another one it gives half of its partial count to the new node. Only when a node has total and partial count that are equal then it can be removed from the network. If a node can't create new nodes, due to having 1 in the partial count, it can just create a new skeleton (new total and partial count) and act as a proxy for that computation
- Reference listing $\to$ a node keeps track of the entities to which it has send a reference through pings, once it has no more references then it will remove itself from the network
### Describe vector clocks in general, compare them with scalar clocks and describe how the former can be used to guarantee causal delivery in a multicast communication system (clarify the assumptions you make)

Vector clocks are a way to keep total ordering inside of a distributed system. They work by keeping track of the timestamp of what a process thinks the state of the network is, all messages will include the vector clock of the process incremented by one. If a process receives a message that can't be accepted then it gets put inside a queue until it can be processed. The difference with scalar clocks is that they don't actually represent the happens-before relationship, vector clocks do. Another difference is that scalar clocks only store one count for all the system. To achieve causal delivery with vector clocks we can just increment the clock value only when accepting the message, not when it's received.
### Describe remote procedure call in general and the various architectural issue that characterize that model of communication

Remote procedure call or RPC refers to a style of distributed computation over the network that hides the network request under normal looking methods. From the point of the programmer, there is no immediate way to know (at least in C) that a method make a network request. These methods only work with copy of the parameters, references are not allowed to be passed over the network. The computation is mostly done synchronously, because it's more in line of a normal computation, but can either be done asynchronously by waiting at least that the receiver sends back an ACK. To create a connection we also need an external registry to connect to that will hold all the remote connections.
### Describe the protocols you know to synchronize clocks in a distributed system

The problem of keeping a global clock for all node inside a distributed system is crucial to it's functioning. To keep physical clocks synchronized we can:
- Ask a centralized server for the correct time with the assumption that these messages travel very fast. 
- Berkley algorithm is a method to keep clocks synchronized by collecting time from all the clients and averaging it, then broadcasting to all the clients.
- NTP $\to$ This is the most popular method and works by having a hierarchical structure that has on top time servers that work on very precise atomic clocks. Each node has to ask for the correct time from the parent node. This algorithm also takes into account the travel time

There are also other approaches for synchronization that make use of logical clocks to achieve a happen before relationship, or even causality within a system. These approaches can either use scalar clocks that hold a counter that represent the state of the computation of the whole system, or vector clock that hold a more fine grained representation of the system using one counter for each process.
### Describe Lamport's protocol to guarantee totally ordered multicast messaging using scalar clocks. Clarify the assumptions

To achieve total order we need to have a system with reliable communication with FIFO links, because the system doesn't tolerate missed messages. Whenever a process communication happen with multicast, this means that to send a message we have to send it to all other processes, when we receive that same message we have to broadcast the ACK and only after receiving all ACKs we can accept it. 
### Consider the architecture of a system for processing large volumes of data in a cluster of machines. Define and compare the following design/architectural choices: batched vs streaming, pipeline vs scheduling. In particular describe their limitations in terms of latency, throughout and elasticity.

A distributed system that processes large volumes of data in clusters will use the map reduce pattern. This architecture is composed of two parts: the map, that processes each single element in the data using some function, and the reduce, that takes the array of results of the map and joins them all together according to some other function. We can have either batched or streaming approaches:
- batches $\to$ The data is processed in chunks of data divided by some scheduler process. This division can lead to latency since we have to wait for data to create these batches, to have high throughput we need to have large batches. This approach is used with a scheduling architecture that is highly elastic, this is because the scheduler can find out what are the slow processes and assign less work to them
- streaming $\to$ The data is processed on the fly, this means that a process doesn't need to wait for some new data to perform the computation, this leads to very low latency and high throughput. This approach is used with pipelining, this means that a process can perform multiple operations within the map function. This can lead to not great network elasticity since it's not easy to predict the resource usage of the network.
### Describe and discuss the kind of failures (the “failure model”) that may happen in a distributed system

There are 3 kinds of failure that can happen in a distributed system
- Omission failures $\to$ when a process makes a request and it never receives a response, be it due to network issues or a crash in the other process
- Byzantine failure $\to$ a process receives a response that has wrong data, because the response was computed in a wrongly manner, (missing steps or non in order computation)
- Timing failure $\to$ a response violates the time constraint that have been put on some computation
### Describe name resolution in presence of structured names

In structured naming resolution we always start from the root node, the root node, that has only knowledge of its direct children, will forward the request to the relevant node and the process repeats until the entity is either found or not while traversing the tree. This process can be performed either iteratively or recursively, we take the DNS name resolution as an example:
- Iterative $\to$ You ask the root server for information about some site, the root will give you back the address of one of its children that can have the site, e.g. if you ask for google.com it will respond with the .com domain server. After receiving this response you will have to ask again for google.com to the .com server that you received in the response and it will proceed in giving you some other server, if it's not connected directly. This means there is a lot of back and forth between the client and the various DNS servers
- Recursive $\to$ You ask the root server for google.com and it asks the .com server for a response and so on until the root server will respond with the result. This means that the client only does one request to the DNS "network" and it will find the result on its own

The main difference in the two approaches, other than the number of network requests, is the ability to cache some intermediate results. In the recursive approach the caching can happen inside the DNS servers and in client.
### Describe scalar clocks in general and how they can be used to implement totally ordered multicast.

Scalar clocks are a way to define the order of operations in a distributed system by means of a counter that represents the "step" a process is on, this is needed because when dealing with a distributed system the notion of a global clock is nonexistent. When dealing with multiple processes, a process will hold its own scalar clock that gets incremented either each time it sends a message or when it receives one. To achieve total ordering we also have to keep track of the process ID when dealing with clocks. To implement a multicast with only scalar clocks we have to assume that each process is connected with a reliable link and has a FIFO queue to receive the messages. When sending the multicast we have to multicast both the message and the ACKs, each and one of them will also hold a timestamp, and only upon receiving all the ACKs can the application collect the message.
### Describe the routing mechanisms of Freenet and Chord (the version with finger tables) and compare them.

Freenet is a distributed network with the sole objective of file sharing and anonymity, and as such the files stored on the network are identified by their cryptographic hash that depends on their content then the file will be stored on some other machine that holds the ids that are closest to the new file. When searching up a file, you make a request to the node that you are connected then it will search up, by guessing the route based on the id, the file traversing the network. With each operation each node can perform caching of the route based on the result. A request finished either when a file is found or when it reaches the max hops. 

In Chord the network is structured by a large DTH that will hold all the routes for all the resources that are published in the system. These hash tables are organized in a logical ring that divide the hashes of all the files that are present on the system. When joining this kind on system we just need to contact another node on the network to find one of the hash table. The main advantage is that they will have guarantees on lookup since resources can only be referenced by a specific table.
### Describe and discuss the mobile code paradigm and the technologies used to implement it

Mobile code is used when we need to distribute the computation on different machines. There are 4 ways to implement:
- Client server $\to$ the client makes a request to the server and the server will respond with the result. This is how most of the internet services work. 
- Remote evaluation $\to$ the client gives some code to execute to the server and then the server will respond with the result. This can be used for large computation for example in the scientific field
- Code on demand $\to$ the client asks the server for some code to run. This is what happens with the javascript that most website use, you visit the site, the site will respond with some code and the you execute it client side
- Mobile agent $\to$ the client will connect to the server and it will perform some computation with the server resources as if they were its own. All the result are stored on the server. This is analogous to using virtual machines within some site.
### Describe and compare the various techniques/algorithms you know to synchronize physical clocks in a distributed system

To synchronize physical clocks in a distributed we have 3 main methods:
- Christian $\to$ Ask a centralized server, this will be the only source of truth of the system
- Berkley $\to$ A coordinator will periodically ask all the clients in a network for their clocks then average them all. Once computed it will send the result to the clients. 
- NTP $\to$ The networks is divided hierarchically and the root will have an atomic clock. All the nodes will ask their parent for the time. NTP is also capable of estimating the offset of clocks of different machines.
### Describe the floodset algorithm (i.e., objective, assumptions, operation) and prove it works correctly.

The floodset algorithm is used for agreement in a distributed system, and it can only work in the presence of faulty processes, not links. It works by duplicating the computation between some nodes and then comparing the result, if even one result is different then we can reject the result. 
