---
tags:
  - compilers
---
[[Regular expressions]] are simple cases of grammars. In fact regexes cannot represent every language, because in order to define a language, one can use rules that, after repeated application, allow to generate all and only the phrases of the language. This set of rules is called a generative grammar. 

The symbols that represent a grammar are:
- **axioms** -> they define the whole language phrase (this doesn't use strings but just phrases)
- **phrases** -> they define the phrase components

Almost all the formal languages of practical use are infinite, that means that in order to generate an infinite language a grammar has to allow recursion if it does not already have a circular derivation.
A grammar that does not contain a recursion is represented by an **acyclic** graph. To better visualize a grammar we use a [[Syntax tree]].

A special case of language used in defining grammars is the [[Dyck language]] where parenthesis are defined to give precedence for operations. 

Two grammars are considered equals only if they generate the same language but they can be divided in two cases:
- weak equivalence -> the generated language is the same
- strong equivalence -> the language is the same and the syntactic tree is structurally equivalent
### Derivation chain
In order to represent the 'creation' of a phrase of a grammar we use a derivation chain that represents the steps took to represent it.

>[!note] Example
>Take for example a grammar that creates palindromes
>$$
\begin{cases}
L = \{ uu^{R} | u \in \{ a,b \}^{*}) \} \\
f \to \epsilon \\
f \to a f a \\
f \to b f b
\end{cases}
>$$
>A valid chain capable of spelling *abba* is $f \to a f a\to abfba \to a b \epsilon b a \to a b b a$

The language generated by starting from a specific non-terminal "A" is $L_{A}(G)$. Also if a phrase contains only terminal characters it is called **sential**
### Backus Normal Form

In order to write grammars we use **BNF** that is composed by four entities
- $V$ -> The non-terminal alphabet like the $f$ above
- $S \in V$ -> A particular non-terminal symbol called **axiom**
- $\Sigma$ -> Is the terminal alphabet or the set of characters that constitutes the phrase
- $P$ -> Is the sets of syntactic rules

>[!warning]
>In order to avoid confusion:
>- metasymbols ("$\to$", " | ", $\dots$) cannot be redifined
>- $V \cup \Sigma = \emptyset$

![[grammar.png]]

### Reduced form

To ensure that every non terminal is defined and that it actually meaningfully contributes to the definition of a language we can define a reduced form from three rules:
- Every non-terminal $A$ is reachable from the axiom
- Every non-terminal $A$ generates a non-empty set of strings
- The grammar may not have inessential circular derivations

This can be achieved through [[Algorithm for reduced BNF]]. Reduced forms however doesn't prohibit redundancy 
### Operations 

The basic regular operations, applied to a free language, still yields free languages.
- Union -> $G = (\Sigma_{1} \cup \Sigma_{2}, \{ S \}\cup V_{1}\cup V_{2}, \{ S \to S_{1} | S_{2} \} \cup P_{1}\cup P_{2}, S)$
- Concatenation -> $G = (\Sigma_{1} \cup \Sigma_{2}, \{ S \}\cup V_{1}\cup V_{2}, \{ S \to S_{1} S_{2} \} \cup P_{1}\cup P_{2}, S)$
- Star -> Just adds the rules $S\to S S_{1} | \epsilon$ 
- Cross -> Just adds the rules $S \to S S_{1}|S_{1}$
### Ambiguity

If a string generated from a grammar can come from at least two different syntactic trees the grammar is considered ambiguous. There can be various types of ambiguity:
1) Two-sided recursion -> a non-terminal symbol that is recursive both on the left and on the right always allows two or more derivations
2) Left-right recursion
3) Union -> if 2 languages share some phrases it's union will surely be ambiguous
4) Concatenation -> if there is a suffix of a phrase of the former language that is a prefix of a phrase in the latter
5) Concatenation of [[Dyck language]]

To resolve this issues the solutions can be to separate some rules to resolve ambiguities.
### Normal form

Normal forms impose more restriction to the rules, but without reducing the family of the generated languages.
- Removing the axiom from the right member of the rules
- Removing copy rules
### Extensions

A free grammar can be extended by means of [[Regular expressions]] since they can be more readable than a grammar. Programming languages are often presented in extended form.

>[!note]
>In an ABNF they arity may be unlimited

This extension is also ambiguous since there are both the ambiguity of the grammar and regexes.

![[chomsky.png]]
### From grammar to [[Push-down stack automaton]]

The initial action is the grammar axiom: the push-down recognizer must check if the source string can be derived from the axiom. Initially the stack contains only the symbol $Z_{0}$ and the axiom $S$, and the input head is positioned on the initial character of the input string. Then it must choose an action that is represented as: 
1) Grammar rules can be viewed as the instructions of a non-deterministic push-down automaton with only one finite state. It uses the stack as a notebook of the sequence of actions to undertake in the next future
2) The stack symbols can be both terminals and non terminals of the grammar. If the stack contains the symbol sequence $A_{1},\dots,A_{k}$ then the automaton executes the action associated with $A_{k}$
3) An action can recursively decomposed in sub-actions

![[PDFSM grammar rules.png]]

>[!lemma]
>The family of free languages generated by free grammars coincides with the family of the languages recognized by one-state push-down automata
### Grammars as a net of [[Finite state automata]]/[[Push-down stack automaton]]

Suppose $G$ is an extended form: each non terminal has one rule $A\to a$ where $a$ is a regular expression over terminals and non terminals. The regexp $a$ defines a regular language and there is a finite automaton that recognizes $a$.

![[net of FSM.png]]

The transitions like $T$ or $F$ can the viewed as the "processing" of the input by the $M_{T}$ and $M_{F}$ respectively. 