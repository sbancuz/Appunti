---
tags:
  - neural_networks
---
### Consider the following 4 activation functions: Sigmoid, Tanh, Linear, ReLU. Discuss for each of them when you should use it and why

These activation functions are used to provide the neurons a non-linearity, any other non-linear function would work as well, making it possible to construct a multi-layer perceptron and be able to represent more complex functions. The tanh and sigmoid functions perform roughly the same role, that is being a binary classifier, in fact the former maps the input to the range $-1$ to $1$, while the latter from $0$ to $1$. The other two functions are used in regression scenarios, with the ReLU being the preferred choice due to its properties in deep neural networks. This is because it solves the vanishing gradient issues that is present in other activation functions due to its derivative being $0$ for negative values and $a$ for positive ones.
### Why is it so important to choose a proper initialization, i.e., what could it happen if it is wrong? Which methods do you suggest for network initialization?

Initialization heavily impacts how the network will learn, or in some extreme cases if it will learn at all. In fact if we initialize all the weights to $0$ the gradients will all be $0$ and nothing will ever change. Apart from all zeros any initialization will provide a suitable result, it will only impact the speed for convergence to that solution, weights too small and the network will  take a lot of time to learn and can also lead to vanishing gradient problems, too big and it can still take a lot of time to converge since it can "jump around" the solution a lot. The preferred method is to use either the He or Xavier initialization techniques that take into account the structure of the neural network. They model the weights as a normal distribution of with variance dependent on the number of inputs and, in case of Xavier, outputs. Transfer learning is also a different way to initialize a neural network, it works by taking the weights of an already pre-trained network, and copying them onto a new one. This is usually done with CNN where we use transfer learning on the backbone that performs feature extraction.