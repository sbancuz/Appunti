---
tags:
  - artificial_intelligence
---
Machine learning is an area of Artificial intelligence focused on building algorithms capable of learning, **extracting** knowledge from experience. These algorithms cannot **create** knowledge. The goal is to build programs that can make informed decisions on new unseen data.

![[Machine learning.png]]

When the target values are real values, we have a regression problem, when they are discrete we have a classification problem.
A simple method for finding a good regression solution is through the use of a [[Perceptron]] 

Suppose we have the experience we collected encoded as data
$$
D=x_{1},\dots ,x_{n}
$$
### Supervised learning

The agent observes input-output pairs and learns a function that maps from input to output. When the agent sees new unseen data it will guess the result according to this function.

More formally given a **training set** of $N$ example input-output pairs where each pair was generated by an unknown function $y = f(x)$ discover a function $h$ that approximates the true function $f$. 
$h$ is called a **hypothesis** about the world, which is a model of the data. We call $y_{i}$ the **ground truth** -- the true answer we are asking our model to predict.

To find the best $h'$ in the hypothesis space we look for a **best fit function** for which each $h(x_{i})$ is close to $y_{i}$.

![[best fit.png]]

The true measure on how well a model performs on the training set, but rather how well it handles unseen data, or **test set**. We say that $h$ generalizes well if it accurately predicts the outputs of the test set. Thus both the piecewise linear function and the 12th degree polynomial, even if perfect in the training data, will  most likely be worst that the linear or sinusoidal.

One way to analyze hypothesis spaces is by the bias they impose and the variance they produce. 
With **bias** we mean the tendency of a predictive hypothesis to deviate from the expected value when averaged over different training sets. In the case of the linear function we might say that the hypothesis is **underfitting** because it fails to provide a pattern in the data.
With **variance** we mean the amount of change in the hypothesis due to fluctuation in the training data. When the variance is too big we call it **overfitting**.

This means that finding the best function for every case is impossible, sometimes an overffited function will underperform in respect to an underfitted one. Most of the time we have to find 
### Unsupervised learning

Exploits regularities in $D$ to build a representation for reasoning or prediction. The most common unsupervised learning task is [[Clustering]].
--> [[9. Data Mining]]
### Reinforcement learning

It performs actions that affect the environment, and receiving rewards it learn to maximize its long-term reward.
The agent needs to compute an action-value function mapping state-action pairs to expected payoffs
$$
Q(a_{t}, s_{t}) = payoff
$$
or state-value functions mapping to expected payoffs.
$$
V( s_{t}) = payoff
$$
Reinforcement learning assumes $Q(\dots)$ to be represented as a table, but in the real world we cannot afford to compute it exactly.

#### Action selection & policy

At each time step, the agent must decide what action to take. And at any given point in time the policy $\pi(st)$ select that very action. So the policy determines how actions map to the new states, and they can be:
- deterministic : this can be modeled as a functions $\pi: S \to A$ 
- stochastic : it maps the probability distribution over the actions $\pi : S,A \to R$

> [!Warning] Exploration-Exploitation dilemma
> 
To obtain a lot of reward, the agent must prefer actions that it has tried in the past and found to lead to high payoff however, to discover such action, it has to try actions that has not selected before. So there needs to be a trade-off between the exploration of new actions and the exploitation of promising ones.

The 2 main type of policies are:
- Greedy policy: that deterministically selects an action with maximal value.
- $\epsilon$-greedy policy: that with a certain $\epsilon$ probability performs a random action otherwise it performs the best one

The environment also has to satisfy the [[Markov property]].

Since the agent has to maximize the reward it receives in the long run
$$
G_{t} \to \infty
$$
so we provide an upper bound to the payoff introducing a discount factor to the future rewards
$$
G_{t} \dot{=} R_{t+1} + \gamma R_{t+2}+ \gamma^{2} R_{t+3}+ \dots + \gamma^{k-1} R_{t+k} + \dots < \infty
$$
Thus the expected reward is defined as
$$
\mathbb  E [G_{k}] = \mathbb E \left[ \sum_{k}^{\infty}\gamma^{k}R_{t+k+1} \right]â‰¤ R_{max} \frac{1}{1-\gamma}
$$
##### Goal and rewards

The action value function and state value function can both be decomposed as the sum of immediate reward
$$
\begin{align}
V(s) &= \mathbb E[r_{t+1} + \gamma V(s_{t+1})|s_{t}= s]  \\
Q(s,a) &= \mathbb E[r_{t+1} + \gamma V(s_{t+1})|s_{t}= s, a_{t} = a] 
\end{align}
$$
At the beginning the table $Q(\dots)$ is initialized with random values, then with time it will populate with
$$
Q(s_{t},a_{t}) = Q(s_{t},a_{t}) + \beta(r_{t+1}+ \gamma \max_{a\in A} Q(s_{t+1},a) - Q(s_{t},a_{t})) 
$$
where $\beta$ is the learning rate.