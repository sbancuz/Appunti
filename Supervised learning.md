---
tags:
  - machine_learning
---
The agent observes input-output pairs and learns a function that maps from input to output. When the agent sees new unseen data it will guess the result according to this function.

More formally given a **training set** of $\mathcal{D} = \{ (x, y) \}$ example input-output pairs where each pair was generated by an unknown function $y = f(x)$ discover a function $h$ that approximates the true function $f$. 
$h$ is called a **hypothesis** about the world, which is a model of the data. We call $y_{i}$ the **ground truth** -- the true answer we are asking our model to predict.

The general method for constructing a supervised learning model are:
- Define a loss function $L$
- Choose an hypothesis space $\mathcal H$ where the real function might be
- Optimize to find the approximate model $h$

>[!warning]
>The problem is that this approach is more **functional approximation** rather then machine learning. This is because we take the $L$ for a given, but in reality it's not. We don't have the true $L$, since if we had it, we would use directly that. So we use an error function to calculate the performance of our model

Supervised learning is often used to resolve these 3 main problems:
- Discrete $\to$ [[Classification]]
- Continuous $\to$ [[Regression]]
- Probability $\to$ Probability estimation

To find the best $h'$ in the hypothesis space we look for a **best fit function** for which each $h(x_{i})$ is close to $y_{i}$.
### Bias-Variance trade-off

![[best fit.png]]

The true measure on how well a model performs on the training set, but rather how well it handles unseen data, or **test set**. We say that $h$ generalizes well if it accurately predicts the outputs of the test set. Thus both the piecewise linear function and the 12th degree polynomial, even if perfect in the training data, will  most likely be worst that the linear or sinusoidal.

One way to analyze hypothesis spaces is by the bias they impose and the variance they produce. 
With **bias** we mean the tendency of a predictive hypothesis to deviate from the expected value when averaged over different training sets. In the case of the linear function we might say that the hypothesis is **underfitting** because it fails to provide a pattern in the data.
With **variance** we mean the amount of change in the hypothesis due to fluctuation in the training data. When the variance is too big we call it **overfitting**.

This means that finding the best function for every case is impossible, sometimes an overffited function will underperform in respect to an underfitted one. Most of the time we have to find a compromise depending on  how appropriate one is from the other. Supervised learning can be done by choosing the hypothesis $h^{*}$ such that is most probable given the data 
$$
h^{*} = \text{argmax}P(data|h)P(h)
$$
### Approximation of the function 

Having $\mathcal D$, to approximate $f$ we need:
- A **loss function** $L$ to minimize that defines how much the approximation is far away from the real function
- An **hypothesis space** $\mathcal H$ that defines the set of functions where you try to approximate the function
- Optimize the loss function to find $h_{1}$ as the best approximation of the real $f$

>[!warning]
>We don't know where the true $f$ lies, so we can't define the loss function. This process is more akin to functional approximation

To fix this function we define the **empirical loss function** with respect to the actual data, this isn't a perfect solution because data will not lead us to the real $f$, but it's still useful. The hypothesis space has to be as small as possible when we have a limited number of samples, this is still because the minimum does not lie on the $f$.
### Model selection

Model selection is the practice of choosing the *better* model in order to avoid over-fitted and under-fitted ones. To avoid over-fitting means that we want to find an approximation with low variance, thus we want a *smoother* function, that is given by the coefficients. To do this we can use **regularization**, a choice of regularization is [[Ridge regression]]. This is the same problem we had in [[System#Selection of model complexity]] for stochastic systems.

>[!Theorem]
>For any learner $L$, the average accuracy is
>$$
> \frac{1}{|\mathcal F|}\sum acc_{G}(L) = \frac{1}{2}
>$$
>This is not a problem since we only focus on a small subset of possible concepts.

Assume that we have a data set $\mathcal D$ with $N$ samples obtained by a function $t_{i} = f(x_{i}) + \epsilon$, now we have
$$
\mathbb  E[\epsilon] = 0, \qquad \text{Var}[\epsilon] = \sigma^{2}
$$
to find a model that approximates $f$ as well as possible we can use the **expected square error**
$$
\mathbb E[(t - y(x))^{2}] = \dots =\text{Var}[t] + \text{Var}[y(x)] + \mathbb E[f(x) - y(x)]^{2}
$$
										^^   $\sigma^{2}$                  ^^                          ^^ *bias^2*
								                       ^^ *variance*

![[bias-variance decomposition.png]]

We can also visualize the total error of a model like

![[model error.png]]

We can clearly see that there is some point where it is better to stop the training, but it's not entirely clear when this point actually is. 

The first, and simplest idea is to just use the train error, this does not work because it will choose the model with the highest number of parameters -- this method is highly correlated with the bias, so it might seem that it's working. 

Another idea is to use the prediction error -- like [[System#Final prediction error criterion]] -- to select the model class. I want to optimize
$$
L_{test} =  \frac{1}{N_{test}} \sum (t_{n} - y(x_{n}))^{2}
$$
But can we estimate this prediction error? It turns out that, given the train error we can try to estimate the test prediction error. This can be done with [[PAC learning]]. 

The are various techniques to used to manage the bias-variance trade-off. The first problem we want to tackle is the one about having a too big of a model. This means that adding more features isn't always the answer since it may lead to high variance and absurdly high training cost -- both in samples size and computational cost.

Let $\mathcal M_{0}$ denote the **null model**, which contains no input feature. For $k=1,\dots,M$
- Fit all combinations of $M$ and $k$ models that contain exactly $k$ features
- Pick the one with lowers $RSS$ 
#### Feature selection

This method is called **feature selection**, it works well but it's too computationally intensive. When $M$ is too large the space of the models will be too big to check. To speed it up we can employ meta-heuristics
- Filter $\to$ rank the features and select the best one
- Embedded $\to$ use some variable selection technique like [[Ridge regression]]
- Wrapper $\to$ add or subtract features until we find the right balance

The resulting model that we want will be the one with the lowest **testing error** and we are sure that will take into account all the different features. So far, we have split the data in two parts: training and testing data. But this is not enough, we also need to have a **validation set** independent from the other two. This validation data will be used to perform model selection after the training data, but before the test!!! 
##### Leave one out cross validation

To maximize the effectiveness of the validation set we have to choose some size of the data to put in it. The first idea, that works well but it's too computationally heavy, is the **leave one out cross validation**. Basically the idea is to remove one element from the training data and that will be the validation set, now compute the usual testing error. Now repeat this procedure $N$ times -- where $N$ is the size of the training set -- and average the result. This is also not the end though, now repeat for all the basis function you want.
##### $k$-fold cross validation

Similar to the leave one out, but now we divide the training data into $k$ equal parts and use those like before as the validation set. This is understandably faster, but a little worse singe it will be a bit biased.

There are a bunch of adjustment techniques, for example
- $C_{p}=\frac{1}{N}(RSS +2d\sigma^{2})$
- $AIC = -2\log L + 2d$ -- see also [[System#Akaike information criterion]]
- $BIC = \frac{1}{N}(RSS + \log(N)d\sigma^{2})$
- $\text{Adjusted }R^{2} = 1- \frac{RSS/(N-d-1)}{TSS/(N-1)}$ where $TSS$ is the total sum of squares
#### Regularization

We have already seen regularization in linear models, in fact it was used in [[Ridge regression]] to shrink the parameter $\lambda$ to $0$. This kind of technique can be used to significantly reduce the variance. Now the problem becomes to find this parameter, or this set of parameters, $\lambda$ to shrink. To do this we can use cross validation and select the parameters with the smallest cross validation error. The all we need to do is to **re-fit** the model. 
#### Dimension reduction

This is an [[Unsupervised learning]] method. Dimension reduction methods **transform** the original features into a smaller set and then retrain the model onto these transformed variables. 
##### Principal component analysis

The idea is to project the input subspace which accounts for most of the **variance**. 
1) Compute the mean $\bar{x}=\frac{1}{N}\sum x_{n}$
2) Compute the covariance matrix $S$
3) Compute the eigenvalues and eigenvectors of $S$ 
4) Take only $\lambda_{k}/\sum\lambda_{i}$ is the proportion of **variance** captured by $k^{th}$ principal component
5) $X'=XE_{k}$ where $E_{k}$ is the matrix with the selected eigenvectors

>[!warning]
>This is dangerous because it may happen that we lose some correlations in the original input space. This is a **lossy compression**
### Model ensembles

These are *meta-techniques* that manage to reduce both the **bias** and the **variance** for free, if you are alright with a long computation. The basic idea is: instead of learning one model, learn several and **combine** them.
#### Bagging

This technique is used to reduce the variance without impacting the bias. The idea here is to average the models, thus reducing variance
$$
Var(\bar{x}) = \frac{Var(x)}{N}
$$
This can work only with independent models. The problem that our models are not, but we can *simulate* $B$ bootstrap samples of the training data created by **random sampling with replacement** -- so after picking one sample at random, but it back in the bag and pick again.
#### Boosting

This technique is used to reduce the bias without impacting the variance. The idea is to sequentially train **weak learners** and summing them up.
1) Weight all train samples equally
2) Train model on that train set
3) Compute the error of the model
4) Increase the weights on train cases where the model goes wrong
5) Repeat

