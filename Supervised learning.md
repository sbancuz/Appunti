---
tags:
  - machine_learning
---
The agent observes input-output pairs and learns a function that maps from input to output. When the agent sees new unseen data it will guess the result according to this function.

More formally given a **training set** of $\mathcal{D} = \{ (x, y) \}$ example input-output pairs where each pair was generated by an unknown function $y = f(x)$ discover a function $h$ that approximates the true function $f$. 
$h$ is called a **hypothesis** about the world, which is a model of the data. We call $y_{i}$ the **ground truth** -- the true answer we are asking our model to predict.

Supervised learning is often used to resolve these 3 main problems:
- Discrete $\to$ [[Classification]]
- Continuous $\to$ [[Regression]]
- Probability $\to$ Probability estimation

To find the best $h'$ in the hypothesis space we look for a **best fit function** for which each $h(x_{i})$ is close to $y_{i}$.

![[best fit.png]]

The true measure on how well a model performs on the training set, but rather how well it handles unseen data, or **test set**. We say that $h$ generalizes well if it accurately predicts the outputs of the test set. Thus both the piecewise linear function and the 12th degree polynomial, even if perfect in the training data, will  most likely be worst that the linear or sinusoidal.

One way to analyze hypothesis spaces is by the bias they impose and the variance they produce. 
With **bias** we mean the tendency of a predictive hypothesis to deviate from the expected value when averaged over different training sets. In the case of the linear function we might say that the hypothesis is **underfitting** because it fails to provide a pattern in the data.
With **variance** we mean the amount of change in the hypothesis due to fluctuation in the training data. When the variance is too big we call it **overfitting**.

This means that finding the best function for every case is impossible, sometimes an overffited function will underperform in respect to an underfitted one. Most of the time we have to find a compromise depending on  how appropriate one is from the other. Supervised learning can be done by choosing the hypothesis $h^{*}$ such that is most probable given the data 
$$
h^{*} = \text{argmax}P(data|h)P(h)
$$
### Approximation of the function 

Having $\mathcal D$, to approximate $f$ we need:
- A **loss function** $L$ to minimize that defines how much the approximation is far away from the real function
- An **hypothesis space** $\mathcal H$ that defines the set of functions where you try to approximate the function
- Optimize the loss function to find $h_{1}$ as the best approximation of the real $f$

>[!warning]
>We don't know where the true $f$ lies, so we can't define the loss function. This process is more akin to functional approximation

To fix this function we define the **empirical loss function** with respect to the actual data, this isn't a perfect solution because data will not lead us to the real $f$, but it's still useful. The hypothesis space has to be as small as possible when we have a limited number of samples, this is still because the minimum does not lie on the $f$.
### Model selection

Model selection is the practice of choosing the *better* model in order to avoid over-fitted and under-fitted ones. To avoid over-fitting means that we want to find an approximation with low variance, thus we want a *smoother* function, that is given by the coefficients. To do this we can use **regularization**, a choice of regularization is [[Ridge regression]].

>[!Theorem]
>For any learner $L$, the average accuracy is
>$$
> \frac{1}{|\mathcal F|}\sum acc_{G}(L) = \frac{1}{2}
>$$
>This is not a problem since we only focus on a small subset of possible concepts.

Assume that we have a data set $\mathcal D$ with $N$ samples obtained by a function $t_{i} = f(x_{i}) + \epsilon$, now we have
$$
\mathbb  E[\epsilon] = 0, \qquad \text{Var}[\epsilon] = \sigma^{2}
$$
to find a model that approximates $f$ as well as possible we can use the **expected square error**
$$
\mathbb E[(t - y(x))^{2}] = \dots =\text{Var}[t] + \text{Var}[y(x)] + \mathbb E[f(x) - y(x)]^{2}
$$
										^^   $\sigma^{2}$                  ^^                          ^^ *bias^2*
								                       ^^ *variance*

![[bias-variance decomposition.png]]


